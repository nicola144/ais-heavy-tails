{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d9ff45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:45:51.466828Z",
     "start_time": "2023-09-14T10:45:50.896808Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d28cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:51:33.323400Z",
     "start_time": "2023-09-14T10:51:31.993627Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import multivariate_normal, multivariate_t, random_correlation, truncnorm\n",
    "import yaml\n",
    "import os\n",
    "import imageio\n",
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23720ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:51:34.836714Z",
     "start_time": "2023-09-14T10:51:34.090158Z"
    }
   },
   "outputs": [],
   "source": [
    "from emukit.core.loop import UserFunctionWrapper\n",
    "from emukit.examples.gp_bayesian_optimization.single_objective_bayesian_optimization import GPBayesianOptimization\n",
    "from emukit.core.loop import UserFunctionResult\n",
    "from emukit.core import ContinuousParameter, InformationSourceParameter, ParameterSpace\n",
    "from emukit.core.initial_designs import RandomDesign\n",
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement, MaxValueEntropySearch, NegativeLowerConfidenceBound\n",
    "import GPy\n",
    "\n",
    "from emukit.core.initial_designs.latin_design import LatinDesign\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd738dd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:51:35.365318Z",
     "start_time": "2023-09-14T10:51:35.359826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load any settings\n",
    "with open(\"settings.yaml\", mode=\"r\") as file:\n",
    "    settings = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39d20b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:51:36.038768Z",
     "start_time": "2023-09-14T10:51:36.034547Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "D = 2\n",
    "ddof_target = settings['ddof_target']\n",
    "\n",
    "ddof_proposal = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072040f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:51:37.142532Z",
     "start_time": "2023-09-14T10:51:37.128033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target\n",
    "mean_target = np.zeros(D)\n",
    "std_devs_target = np.diag(np.sqrt(np.ones(D)*2))\n",
    "random_corr_mat = random_correlation(eigs=np.ones(D)).rvs(1)\n",
    "cov_target =  std_devs_target @ random_corr_mat  @ std_devs_target\n",
    "shape_target = ((ddof_target - 2) / ddof_target) * cov_target\n",
    "\n",
    "# Proposal\n",
    "mean_proposal = np.ones(D)*2\n",
    "\n",
    "std_devs_proposal = np.diag(np.sqrt(np.ones(D)*7))\n",
    "# For now same correlation as target, just different variances\n",
    "cov_proposal = std_devs_proposal @ random_corr_mat @ std_devs_proposal\n",
    "\n",
    "# The Student-t 'shape matrix' is a rescaled covariance\n",
    "shape_proposal = ((ddof_proposal - 2) / ddof_proposal) * cov_proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e457e2",
   "metadata": {},
   "source": [
    "We implement an escort moment-matching procedure without any temporal mixture and with fixed degrees of freedom. This will be our building bolck for degrees of freedom adaptation.\n",
    "\n",
    "Consider a family of Student distribution with $\\nu$ degrees of freedom.\n",
    "Set $\\alpha = 1 + \\frac{2}{\\nu + d}$ and initial location and scatter parameters $\\mu_0, \\Sigma_0$.\n",
    "\n",
    "For $t=1,\\dots,T$:\n",
    "\n",
    "\n",
    "* Set $\\alpha_t = 1 + \\frac{2}{\\nu_t + d}$.\n",
    "\n",
    "* Weight the samples from $q(\\cdot ; \\theta_t)$ (which have already been sampled) using the $\\alpha_{t}$-escort of the target: $w_{t}^{(m)} = \\frac{\\pi(x_{t}^{(m)})^{\\alpha_{t}}}{ q(x_{t}^{(m)}; \\theta_{t})} $ for $m=1,\\dots,M$\n",
    "* Normalize the weights: $\\bar{w}_{t}^{(m)} = \\frac{w_{t}^{(m)}}{\\sum_{m=1}^{M} w_{t}^{(m)}}$\n",
    "\n",
    "\n",
    "* Update proposal parameters:\n",
    "    * $\\mu_{t+1} = \\sum_{m=1}^{M} \\bar{w}_t^{(m)} x_{t}^{(m)}$\n",
    "    * $\\Sigma_{t+1} = \\sum_{m=1}^{M} \\bar{w}_{t}^{(m)}  (x_{t}^{(m)} - \\mu_{t+1})(x_{t}^{(m)} - \\mu_{t+1})^\\top $\n",
    "    \n",
    "  With these weights this corresponds to doing an escort moment-matching update with parameter $\\alpha_t$.\n",
    "\n",
    "* Draw $M$ samples from the novel proposal: $x_{t+1}^{(m)} \\sim q(x, \\theta_{t+1}), m=1,\\dots,M$\n",
    "\n",
    "* Weight the samples using the target: $w_t^{(m)} = \\frac{\\pi(x_{t}^{(m)})}{ q(x_{t}^{(m)}; \\theta_t)} $ for $m=1,\\dots,M$\n",
    "* Normalize the weights: $\\bar{w}_{t+1}^{(m)} = \\frac{w_{t+1}^{(m)}}{\\sum_{m=1}^{M} w_{t+1}^{(m)}}$\n",
    "* Compute the $\\alpha_t$-ESS: $ESS_{\\alpha_t} = \\left(\\sum_{m=1}^M (\\bar{w}_{t+1}^{(m)})^{\\alpha_t}\\right)^{\\frac{1}{1-\\alpha_t}}$.\n",
    "* Use all the values $\\{ \\nu_{\\tau}, ESS_{\\alpha_{\\tau}} \\}_{\\tau=1}^t$ to set $\\nu_{t+1}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03596ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escortMM_adaptive_dof(mu_initial,shape_initial, n_iterations, target_pdf, ddof_proposal, M=200):\n",
    "\n",
    "    # Map of available acquisition functions\n",
    "    acquisition_functions = {\n",
    "        'EI': ExpectedImprovement,\n",
    "        'LCB': NegativeLowerConfidenceBound,\n",
    "        'MES': MaxValueEntropySearch\n",
    "    }\n",
    "\n",
    "    # Define parameter space for Bayesian optimization\n",
    "    dof_proposal_space = ParameterSpace([ContinuousParameter('dof_proposal', 1, 100)])\n",
    "\n",
    "    # Initialize Bayesian optimization\n",
    "    gpy_model = GPy.models.GPRegression(np.array([[dof_proposal_initial]]), np.array([[objective_function([dof_proposal_initial])]]))\n",
    "    emukit_model = GPyModelWrapper(gpy_model)\n",
    "    acquisition = acquisition_functions[acquisition_function](emukit_model)\n",
    "    bayesopt_loop = BayesianOptimizationLoop(model = emukit_model, space = dof_proposal_space, acquisition = acquisition)\n",
    "\n",
    "\n",
    "    observed_ddof = []\n",
    "    observed_ess = []\n",
    "    \n",
    "    # Iteration 0\n",
    "    first_proposal = multivariate_t(loc=mu_initial,shape=shape_initial,df=ddof_proposal)\n",
    "    \n",
    "    # plot_contour_lines(first_proposal, target_pdf, iteration=0)\n",
    "    \n",
    "    samples_current = first_proposal.rvs(size=M)\n",
    "    \n",
    "    log_numerator = target_pdf.logpdf(samples_current)\n",
    "    \n",
    "    log_denominator = first_proposal.logpdf(samples_current) # No temporal mixture in iteration 0\n",
    "\n",
    "    # assert log_numerator.shape == log_denominator.shape\n",
    "    current_logweights = log_numerator - log_denominator\n",
    "\n",
    "    normalized_logweights = current_logweights - logsumexp(current_logweights)\n",
    "\n",
    "    # Iteration t > 0\n",
    "    for t in tqdm(range(1,n_iterations)):\n",
    "        \n",
    "        alpha = 1 + 2 / (ddof_proposal + D)\n",
    "        \n",
    "        ## Update the proposal\n",
    "        logweights_update = alpha*log_numerator - log_denominator\n",
    "        normalized_logweights_update = logweights_update - logsumexp(logweights_update)\n",
    "        \n",
    "        mu_current = np.zeros(D)\n",
    "        for m in range(M):\n",
    "            mu_current += np.exp(normalized_logweights_update[m]) * samples_current[m, :]\n",
    "        \n",
    "        shape_current = np.zeros((D, D))\n",
    "        for m in range(M):\n",
    "            shape_current += np.exp(normalized_logweights_update[m]) * (samples_current[m, :].reshape(-1, 1) @ samples_current[m, :].reshape(1, -1))\n",
    "        \n",
    "        current_proposal = multivariate_t(loc=mu_current, shape=shape_current, df=ddof_proposal)\n",
    "         \n",
    "        \n",
    "        # Plot current proposal vs target\n",
    "        # plot_contour_lines(current_proposal, target_pdf, iteration=t)\n",
    "        \n",
    "        ## Sampling from the new proposal\n",
    "        samples_current = current_proposal.rvs(size=M)\n",
    "    \n",
    "        log_numerator = target_pdf.logpdf(samples_current)\n",
    "    \n",
    "        log_denominator = current_proposal.logpdf(samples_current)\n",
    "\n",
    "        ## Evaluate the next ddof\n",
    "        \n",
    "        logweights_ess = log_numerator - log_denominator\n",
    "        normalized_logweights_ess = logweights_ess - logsumexp(logweights_ess)\n",
    "\n",
    "        ESS_alpha = sum(np.exp(alpha*normalized_logweights_ess))**(1 / (1-alpha))\n",
    "        \n",
    "        observed_ddof.append(ddof_proposal)\n",
    "        observed_ess.append(ESS_alpha)\n",
    "        \n",
    "        if t <= 3:\n",
    "            #we accumulate a few observations with randomly chosen ddof before starting the Bayesian optimization\n",
    "            rv = truncnorm(-ddof_proposal, 100)\n",
    "            ddof_proposal = rv.rvs() + ddof_proposal\n",
    "            \n",
    "        else:\n",
    "            #we have enough data points to start optimizing in a Bayesian fashion\n",
    "            space = ParameterSpace([ContinuousParameter(\"x\", 0, 20)])\n",
    "\n",
    "            # results = None\n",
    "            # bo = GPBayesianOptimization(variables_list=space.parameters, X=observed_ddof, Y=observed_ess)\n",
    "            # x_new = bo.get_next_points(results)\n",
    "\n",
    "            model_gpy = GPRegression(observed_ddof,observed_ess) # Train and wrap the model in Emukit\n",
    "            model_emukit = GPyModelWrapper(model_gpy)\n",
    "\n",
    "            model_gpy.likelihood.variance.fix()\n",
    "\n",
    "            model_gpy.optimize()\n",
    "\n",
    "            safe_gp_optimization(model_gpy)\n",
    "\n",
    "            expected_improvement = ExpectedImprovement(model = model_emukit)\n",
    "\n",
    "        # Update Bayesian optimization with new data point and get new dof_proposal\n",
    "        bayesopt_loop.iteration_end(np.array([[dof_proposal]]), np.array([[objective_function([dof_proposal])]]))\n",
    "        dof_proposal_config, _ = bayesopt_loop.get_next_points(results=None)\n",
    "        dof_proposal = dof_proposal_config[0][0]\n",
    "\n",
    "\n",
    "\n",
    "            # ddof_proposal = x_new\n",
    "            \n",
    "    return observed_ddof\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21022666",
   "metadata": {},
   "outputs": [],
   "source": [
    "escortMM_adaptive_dof(mu_initial=mean_proposal,shape_initial=shape_proposal, n_iterations=30, target_pdf=multivariate_t(loc=mean_target,shape=shape_target,df=ddof_target), ddof_proposal=ddof_proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fe4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
